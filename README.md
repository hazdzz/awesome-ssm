# SSMs and related works list

## About
A list for SSMs and related works.

## List for SSMs
| Number | SSM | Paper | Code | Conference or Journal | URL |
|:------:|:--------------------------:|-------|------|:-------:|----------------------------------------|
| 1 | HiPPO | HiPPO: Recurrent Memory with Optimal Polynomial Projections | https://github.com/state-spaces/s4 | NeurIPS 2020 | https://proceedings.neurips.cc/paper/2020/hash/102f0bb6efb3a6128a3c750dd16729be-Abstract.html |
| 2 | LSSL | Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers | https://github.com/state-spaces/s4 | NeurIPS 2021 | https://openreview.net/forum?id=yWd42CWN3c |
| 3 | S4 | Efficiently Modeling Long Sequences with Structured State Spaces | https://github.com/state-spaces/s4 | ICLR 2022 | https://openreview.net/forum?id=uYLFoz1vlAC |
| 4 | DSS | Diagonal State Spaces are as Effective as Structured State Spaces | https://github.com/ag1988/dss | NeurIPS 2022 | https://openreview.net/forum?id=RjS0j6tsSrf |
| 5 | S4D | On the Parameterization and Initialization of Diagonal State Space Models | https://github.com/state-spaces/s4 | NeurIPS 2022 | https://openreview.net/forum?id=yJE7iQSAep |
| 6 | Generalized HiPPO | How to Train your HIPPO: State Space Models with Generalized Orthogonal Basis Projections | https://github.com/state-spaces/s4 | ICLR 2023 | https://openreview.net/forum?id=klK17OQ3KB |
| 7 | GSS | Long Range Language Modeling via Gated State Spaces | | ICLR 2023 | https://openreview.net/forum?id=5MkYIYCbva |
| 8 | Liquid S4 | Liquid Structural State-Space Models | https://github.com/raminmh/liquid-s4 | ICLR 2023 | https://openreview.net/forum?id=g4OTKRKfS7R |
| 9 | S5 | Simplified State Space Layers for Sequence Modeling | https://github.com/lindermanlab/S5 | ICLR 2023 | https://openreview.net/forum?id=Ai8Hw3AXqks |
| 10 | H3 | Hungry Hungry Hippos: Towards Language Modeling with State Space Models | https://github.com/HazyResearch/H3 | ICLR 2023 | https://openreview.net/forum?id=COZDy0WYGg |
| 11 | S4-PTD and S5-PTD | Robustifying State-space Models for Long Sequences via Approximate Diagonalization | | ICLR 2024 | https://openreview.net/forum?id=DjeQ39QoLQ |
| 12 | STU | Spectral State Space Models | https://github.com/catid/spectral_ssm | | https://arxiv.org/abs/2312.06837 |
| 13 | S6 | Mamba: Linear-Time Sequence Modeling with Selective State Spaces | https://github.com/state-spaces/mamba | | https://arxiv.org/abs/2312.00752 |

## List for Linear RNNs (LRNNs)
| Number | LRNN | Paper | Code | Conference or Journal | URL |
|:------:|:--------------------------:|-------|------|:-------:|----------------------------------------|
| 1 | Mega | Mega: Moving Average Equipped Gated Attention | https://github.com/facebookresearch/mega | ICLR 2023 | https://openreview.net/forum?id=qNLe3iq2El |
| 2 | Hyena | Hyena Hierarchy: Towards Larger Convolutional Language Models | https://github.com/hazyresearch/safari | ICML 2023 | https://proceedings.mlr.press/v202/poli23a.html | 
| 3 | RWKV | RWKV: Reinventing RNNs for the Transformer Era | https://github.com/BlinkDL/RWKV-LM | EMNLP 2023 | https://aclanthology.org/2023.findings-emnlp.936/ |
| 4 | RetNet | Retentive Network: A Successor to Transformer for Large Language Models | https://github.com/microsoft/torchscale | | https://arxiv.org/abs/2307.08621 |
| 5 | Monarch Mixer | Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture | https://github.com/HazyResearch/m2 | NeurIPS 2023 | https://openreview.net/forum?id=cB0BImqSS9 |
| 6 | SeqBoat | Sparse Modular Activation for Efficient Sequence Modeling | https://github.com/renll/SeqBoat | NeurIPS 2023 | https://openreview.net/forum?id=TfbzX6I14i |
| 7 | GLA Transformer | Gated Linear Attention Transformers with Hardware-Efficient Training | https://github.com/sustcsonglin/flash-linear-attention | | https://arxiv.org/abs/2312.06635 |
